{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snapchat Political Ads\n",
    "* **See the main project notebook for instructions to be sure you satisfy the rubric!**\n",
    "* See Project 03 for information on the dataset.\n",
    "* A few example prediction questions to pursue are listed below. However, don't limit yourself to them!\n",
    "    * Predict the reach (number of views) of an ad.\n",
    "    * Predict how much was spent on an ad.\n",
    "    * Predict the target group of an ad. (For example, predict the target gender.)\n",
    "    * Predict the (type of) organization/advertiser behind an ad.\n",
    "\n",
    "Be careful to justify what information you would know at the \"time of prediction\" and train your model using only those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Findings\n",
    "\n",
    "\n",
    "### Introduction\n",
    "This project will focus on predicting the country of origin of an ad. This will be a classification problem since the goal is to predict a categorical variable, rather than a continuous variable. In order to predict the country of origin, this project will use CountryCode as the target variable. The objective will be to maximize the accuracy of the model. Given the malicious ad campaigns that have taken place in the US by foreign governments, this project asks whether it is feasible to know where an ad originated from, with only knowing some basic information of the ad. For the purpose of addressing this question we will exclude billing address from the data as well.\n",
    "\n",
    "### Baseline Model\n",
    "For a baseline model I decided to first clean up my data a little. I first filled null values with their appropriate meanings based on snapchat's readme file for the data. As well, I excluded columns that had over 95% null values as I do not think they would provide a better prediction. I ended up using 16 feature variables. Of the 16, 14 of the features were nominal and 2 features were quantitative. As a baseline model, I decided to one hot encode all of the features except the two quantitative features. I used a Random Forest Classifier and was able to achieve an average accuracy of approximately 63%. I used cross validation as well as an accuracy score based on a held out test sample of the data to make sure my accuracy was consistent. For a baseline model, I feel this is a decent outcome since it is better than just randomly guessing the country of origin. Also, it is decently high for just one hot encoding the original features, so I believe there is a lot of room for improvement.\n",
    "\n",
    "### Final Model\n",
    "I ended up adding two new features to my data. I first chose to create a column called Days which calculates the time the ad was shown on snapchat by subtracting the start date from the end date. I think this column might be helpful in predictions in case there are associations with the length of time an ad is shown and the country the ad is from. Also, I wanted to make better use of my start date and end date data, rather than just one hot encoding them.  \n",
    "  \n",
    "I also changed the age bracket column to include only 5 different groups. I did this because originally the age bracket column contained all sorts of values and in different formats. I thought this column might have an association with the country of origin so I used regex to extract the age groups and bin the ages into fewer categories. This also works better for using one hot encoding.  \n",
    "  \n",
    "I ended up choosing a random forest classifier model for my data. I also tried a k nearest neighbor approach, but the results were not nearly as good. I ran grid search on my random forest model and ended up finding the best features to be: 'criterion': 'gini','max_depth': 10,'max_features': 'sqrt','min_samples_leaf': 1, 'min_samples_split': 3,'n_estimators': 100. My method of model selection included evaluating two different classifiers, random forest and knn, then performing grid search on both and calculating accuracy scores based on cross validation as well as a held out test sample. What I found was that the random forest classifier outperformed the knn classifier by over 20%. In the end I was able to improve my baseline model by slightly over 10% up to around 74% accuracy. \n",
    "\n",
    "### Fairness Evaluation\n",
    "To evaluate fairness I split my dataset into two subsets. The data is split by impressions with one subset being the ads with number of impressions that are below the median number of impressions for all ads. The other subset is ads with number of impressions that are equal to or greater than the median number of impressions for all ads. Since my target variable is categorical, it does not make sense to choose a parity measure, so I used accuracy. I then performed a permutation test between my two subsets and used the difference of means for my test statistic. My hypotheses are outlined below:  \n",
    "  \n",
    "**Null Hypothesis:** The model predicts with the same accuracy for ads with high number of impressions as ads with low number of impressions.  \n",
    "  \n",
    "**Alternative Hypothesis:** The model predicts with a different accuracy for ads with a high number of impressions as ads with low number of impressions.\n",
    "\n",
    "**Note:** We define high number of impressions to be ads with impressions equal to or greater than the median impressions for all ads, and low number of impressions is ads with impressions less than the median.  \n",
    "  \n",
    "**Decision:** We will set a significance level of .05. So we will reject the null hypothesis for a p value less than .05 and fail to reject the null hypothesis if it is greater than .05.  \n",
    "  \n",
    "After running my hypothesis test I got a p value of .43 so I fail to reject my null hypothesis. Thus, between ads with high number of impressions and low number of impressions my model predicts both groups fairly equally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # Higher resolution figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data \n",
    "ads_fp = os.path.join('data', 'PoliticalAds2018.csv')\n",
    "ads2018 = pd.read_csv(ads_fp)\n",
    "ads_fp = os.path.join('data', 'PoliticalAds2019.csv')\n",
    "ads2019 = pd.read_csv(ads_fp)\n",
    "ads = ads2018.append(ads2019)\n",
    "orig_ads = ads.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing null values with actual value the null is representing\n",
    "ads['Gender'] = ads['Gender'].fillna('ALL')\n",
    "ads['AgeBracket'] = ads['AgeBracket'].fillna('-1')\n",
    "ads['Interests'] = ads['Interests'].fillna('NONE')\n",
    "ads['Language'] = ads['Language'].fillna('NONE')\n",
    "ads['CandidateBallotInformation'] = (ads['CandidateBallotInformation'].\n",
    "                                     fillna('NONE'))\n",
    "ads['Regions (Included)'] = ads['Regions (Included)'].fillna('NONE')\n",
    "ads['Radius Targeting (Included)'] = (ads['Radius Targeting (Included)'].\n",
    "                                      fillna('NONE'))\n",
    "ads['Postal Codes (Included)'] = (ads['Postal Codes (Included)'].\n",
    "                                  fillna('NONE'))\n",
    "ads['EndDate'] = ads['EndDate'].fillna('ONGOING')\n",
    "ads['Segments'] = ads['Segments'].fillna('NONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that have over 95 % null values\n",
    "cols = ['Regions (Excluded)', 'Electoral Districts (Excluded)', \n",
    "        'Electoral Districts (Included)', 'Radius Targeting (Excluded)', \n",
    "        'Metros (Excluded)', 'Metros (Included)', 'Postal Codes (Excluded)', \n",
    "        'Location Categories (Excluded)', 'Location Categories (Included)', \n",
    "        'OsType', 'AdvancedDemographics', 'Targeting Connection Type', \n",
    "        'Targeting Carrier (ISP)', 'CreativeUrl','CreativeProperties']\n",
    "ads = ads.drop(cols, axis=1)\n",
    "# Drop ADID since it is a unique value for each ad\n",
    "ads = ads.drop(['ADID'], axis=1)\n",
    "# Dropping billing address since the point of the model is to predict \n",
    "# origin location\n",
    "ads = ads.drop('BillingAddress', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing baseline features to use and creating a transformer to hold \n",
    "# those features\n",
    "transformer = ColumnTransformer([\n",
    "    ('one_hot_curr', OneHotEncoder(), ['Currency Code']),\n",
    "    ('one_hot_start', OneHotEncoder(handle_unknown='ignore'), \n",
    "     ['StartDate']),\n",
    "    ('one_hot_end', OneHotEncoder(handle_unknown='ignore'), \n",
    "     ['EndDate']),\n",
    "    ('one_hot_org', OneHotEncoder(handle_unknown='ignore'), \n",
    "     ['OrganizationName']),\n",
    "    ('one_hot_cand', OneHotEncoder(handle_unknown='ignore'), \n",
    "     ['CandidateBallotInformation']),\n",
    "    ('one_hot_pay', OneHotEncoder(handle_unknown='ignore'), \n",
    "     ['PayingAdvertiserName']),\n",
    "    ('one_hot_gender', OneHotEncoder(), ['Gender']),\n",
    "    ('one_hot_age', OneHotEncoder(handle_unknown='ignore'), \n",
    "     ['AgeBracket']),\n",
    "    ('one_hot_regions', OneHotEncoder(handle_unknown='ignore'), \n",
    "     ['Regions (Included)']),\n",
    "    ('one_hot_radius', OneHotEncoder(handle_unknown='ignore'), \n",
    "     ['Radius Targeting (Included)']),\n",
    "    ('one_hot_postal', OneHotEncoder(handle_unknown='ignore'), \n",
    "     ['Postal Codes (Included)']),\n",
    "    ('one_hot_int', OneHotEncoder(handle_unknown='ignore'), \n",
    "     ['Interests']),\n",
    "    ('one_hot_seg', OneHotEncoder(handle_unknown='ignore'), \n",
    "     ['Segments']),\n",
    "    ('one_hot_lan', OneHotEncoder(handle_unknown='ignore'), \n",
    "     ['Language'])\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_accuracy': 0.6049960967993755,\n",
       " 'validation_accuracy': 0.6370903632402711}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Creating a pipeline to process transformations and execute random \n",
    "# forest tree\n",
    "pl = Pipeline([\n",
    "        ('transformations', transformer),\n",
    "        ('rf', RandomForestClassifier(bootstrap=True, \n",
    "                class_weight=None, criterion='gini',\n",
    "                max_depth=5, max_features='auto', \n",
    "                max_leaf_nodes=None, min_samples_leaf=1, \n",
    "                min_samples_split=2, min_weight_fraction_leaf=0.0, \n",
    "                n_estimators=100, n_jobs=1, oob_score=False, \n",
    "                random_state=None, verbose=0,warm_start=False))\n",
    "])\n",
    "scores = dict()\n",
    "# Getting the X and y data\n",
    "X = ads.drop('CountryCode', axis=1)\n",
    "y = ads['CountryCode']\n",
    "# Getting the training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "# Fitting the data\n",
    "pl.fit(X_train, y_train)\n",
    "# Getting the accuracy score\n",
    "scores['test_accuracy'] = (accuracy_score(y_test, pl.predict(X_test)))\n",
    "scores['validation_accuracy'] = np.mean(cross_val_score(\n",
    "    pl, X_train, y_train, cv=4))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's add some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing 'EndDate' null values with the current date to represent \n",
    "# they are still running the ad\n",
    "replace = str(datetime.now())\n",
    "ads['EndDate'] = orig_ads['EndDate'].fillna(replace)\n",
    "# Using 'StartDate' and EndDate to create a column 'Days' that denotes \n",
    "# the number of days the ad has been showing\n",
    "duration = ((pd.to_datetime(ads['EndDate']) - pd.to_datetime(\n",
    "    ads['StartDate']))).dt.days\n",
    "ads['Days'] = duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_ages(nums):\n",
    "    if int(nums[0]) == -1:\n",
    "        return 'all'\n",
    "    elif int(nums[0]) >= 18 & len(nums) > 1:\n",
    "        if int(nums[1]) < 40:\n",
    "            return 'young_adult'\n",
    "        if int(nums[1]) >= 40:\n",
    "            return 'adult'\n",
    "    elif int(nums[0]) >= 18 & len(nums) == 1:\n",
    "        return 'adult'\n",
    "    elif int(nums[0]) < 18 & len(nums) > 1:\n",
    "        if int(nums[1]) < 18:\n",
    "            return 'child'\n",
    "        if int(nums[1]) > 18 & int(nums[1]) < 40:\n",
    "            return 'child_young_adult'\n",
    "        if int(nums[1]) > 18 & int(nums[1]) >= 40:\n",
    "            return 'all'\n",
    "    elif int(nums[0]) < 18 & len(nums) == 1:\n",
    "        return 'child'\n",
    "    else:\n",
    "        return 'adult'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean up and categorize the age brackets into child (<18), \n",
    "# young adult (>18 and < 40), adult (>18 and >40), child young \n",
    "# adult (<18 and <40), and all \n",
    "brackets = ads['AgeBracket'].apply(lambda x: re.findall(r'\\d+', x))\n",
    "brackets = brackets.apply(bin_ages)\n",
    "ads['AgeBracket'] = brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6479313036690086"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the same transformer as above, but now we have the two \n",
    "# added features to build a pipeline\n",
    "# Creating a pipeline to process transformations and execute \n",
    "# random forest tree\n",
    "pl = Pipeline([\n",
    "        ('transformations', transformer),\n",
    "        ('rf', RandomForestClassifier(bootstrap=True, \n",
    "                class_weight=None, criterion='gini',\n",
    "                max_depth=5, max_features='auto', \n",
    "                max_leaf_nodes=None, min_samples_leaf=1, \n",
    "                min_samples_split=2, min_weight_fraction_leaf=0.0, \n",
    "                n_estimators=100, n_jobs=1, oob_score=False, \n",
    "                random_state=None, verbose=0,warm_start=False))\n",
    "])\n",
    "# Getting the X and y data along with training and testing data\n",
    "X = ads.drop('CountryCode', axis=1)\n",
    "y = ads['CountryCode']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3)\n",
    "# Fitting the data\n",
    "pl.fit(X_train, y_train)\n",
    "# Getting the accuracy score\n",
    "accuracy_score(y_test, pl.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run grid search to find the best model fit\n",
    "parameters1 = {'rf__n_estimators': [95, 100], \n",
    "              'rf__max_features': ['log2', 'sqrt','auto'], \n",
    "              'rf__criterion': ['entropy', 'gini'],\n",
    "              'rf__max_depth': [3, 5, 7, 10], \n",
    "              'rf__min_samples_split': [2, 3, 5],\n",
    "              'rf__min_samples_leaf': [1,5,8]\n",
    "             }\n",
    "grid = GridSearchCV(pl, parameters1, cv=3)\n",
    "grid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__criterion': 'gini',\n",
       " 'rf__max_depth': 10,\n",
       " 'rf__max_features': 'auto',\n",
       " 'rf__min_samples_leaf': 1,\n",
       " 'rf__min_samples_split': 3,\n",
       " 'rf__n_estimators': 100}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_accuracy': 0.7439500390320063, 'validation_accuracy': 0.757960348992036}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets take the best parameters and use them in our model\n",
    "pl = Pipeline([\n",
    "        ('transformations', transformer),\n",
    "        ('rf', RandomForestClassifier(bootstrap=True, \n",
    "                class_weight=None, criterion='gini',\n",
    "                max_depth=10, max_features='sqrt', \n",
    "                max_leaf_nodes=None, min_samples_leaf=1, \n",
    "                min_samples_split=3, min_weight_fraction_leaf=0.0, \n",
    "                n_estimators=100, n_jobs=1, oob_score=False, \n",
    "                random_state=None, verbose=0,warm_start=False))\n",
    "])\n",
    "scores = dict()\n",
    "# Getting the X and y data\n",
    "X = ads.drop('CountryCode', axis=1)\n",
    "y = ads['CountryCode']\n",
    "# Getting the training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "# Fitting the data\n",
    "pl.fit(X_train, y_train)\n",
    "# Getting the accuracy score\n",
    "scores['test_accuracy'] = (accuracy_score(y_test, pl.predict(X_test)))\n",
    "scores['validation_accuracy'] = np.mean(cross_val_score(\n",
    "    pl, X_train, y_train, cv=4))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressively, just by adding our two new features and running Grid Search cross validation we are able to consistently improve our model by over 10 percentage points in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a different classifier now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_accuracy': 0.419984387197502, 'validation_accuracy': 0.4385737767872204}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying knn\n",
    "pl = Pipeline([\n",
    "        ('transformations', transformer),\n",
    "        ('knn', KNeighborsClassifier())\n",
    "])\n",
    "scores = dict()\n",
    "# Getting the X and y data\n",
    "X = ads.drop('CountryCode', axis=1)\n",
    "y = ads['CountryCode']\n",
    "# Getting the training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "# Fitting the data\n",
    "pl.fit(X_train, y_train)\n",
    "# Getting the accuracy score\n",
    "scores['test_accuracy'] = (accuracy_score(y_test, pl.predict(X_test)))\n",
    "scores['validation_accuracy'] = np.mean(cross_val_score(\n",
    "    pl, X_train, y_train, cv=4))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use grid search to see if we can improve it\n",
    "parameters2 = {'knn__n_neighbors':[2,3,5,7,9,11],\n",
    "              'knn__weights':['uniform', 'distance'],\n",
    "              'knn__algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "              'knn__leaf_size':[25, 30, 35],}\n",
    "grid = GridSearchCV(pl, parameters2, cv=3)\n",
    "grid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knn__algorithm': 'auto',\n",
       " 'knn__leaf_size': 25,\n",
       " 'knn__n_neighbors': 11,\n",
       " 'knn__weights': 'uniform'}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_accuracy': 0.4769711163153786,\n",
       " 'validation_accuracy': 0.48912405654790747}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the found parameters\n",
    "pl = Pipeline([\n",
    "        ('transformations', transformer),\n",
    "        ('rf', KNeighborsClassifier(algorithm='auto',\n",
    "                                   leaf_size=25,\n",
    "                                   n_neighbors=11,\n",
    "                                   weights='uniform'))\n",
    "])\n",
    "scores = dict()\n",
    "# Getting the X and y data\n",
    "X = ads.drop('CountryCode', axis=1)\n",
    "y = ads['CountryCode']\n",
    "# Getting the training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "# Fitting the data\n",
    "pl.fit(X_train, y_train)\n",
    "# Getting the accuracy score\n",
    "scores['test_accuracy'] = (accuracy_score(y_test, pl.predict(X_test)))\n",
    "scores['validation_accuracy'] = np.mean(cross_val_score(\n",
    "    pl, X_train, y_train, cv=4))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not able to get very good results using knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model let's evaluate the performance on two subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the median to split the ads into two subsets\n",
    "med = ads['Impressions'].median()\n",
    "low_ads = ads[ads['Impressions'] < med]\n",
    "high_ads = ads[ads['Impressions'] >= med]\n",
    "X = ads.drop('CountryCode', axis=1)\n",
    "y = ads['CountryCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'high': 0.7812646370023419, 'low': 0.7782466010314112}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets find the accuracy of low and high by retraining the model\n",
    "scores = dict()\n",
    "pl = Pipeline([\n",
    "        ('transformations', transformer),\n",
    "        ('rf', RandomForestClassifier(bootstrap=True, \n",
    "                class_weight=None, criterion='gini',\n",
    "                max_depth=10, max_features='sqrt', \n",
    "                max_leaf_nodes=None, min_samples_leaf=1, \n",
    "                min_samples_split=3, min_weight_fraction_leaf=0.0, \n",
    "                n_estimators=100, n_jobs=1, oob_score=False, \n",
    "                random_state=None, verbose=0,warm_start=False))\n",
    "])\n",
    "pl.fit(X, y)\n",
    "# Getting the X and y data \n",
    "X = low_ads.drop('CountryCode', axis=1)\n",
    "y = low_ads['CountryCode']\n",
    "\n",
    "# Getting the accuracy score\n",
    "accuracy = (accuracy_score(y, pl.predict(X)))\n",
    "scores['low'] = accuracy\n",
    "\n",
    "# Now find accuracy of high\n",
    "# Getting the X and y data \n",
    "X = high_ads.drop('CountryCode', axis=1)\n",
    "y = high_ads['CountryCode']\n",
    "# Getting the accuracy score\n",
    "accuracy = (accuracy_score(y, pl.predict(X)))\n",
    "scores['high'] = accuracy\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform a permutation test on the high and low data to see if there is a significant difference between the accuracies. I will define my test statistic as the difference in accuracy.  \n",
    "  \n",
    "**Null Hypothesis:** The model predicts with the same accuracy for ads with high number of impressions as ads with low number of impressions.  \n",
    "  \n",
    "**Alternative Hypothesis:** The model predicts with a different accuracy for ads with a high number of impressions as ads with low number of impressions.\n",
    "\n",
    "**Note:** We define high number of impressions to be ads with impressions equal to or greater than the median impressions for all ads, and low number of impressions is ads with impressions less than the median.  \n",
    "  \n",
    "**Decision:** We will set a significance level of .05. So we will reject the null hypothesis for a p value less than .05 and fail to reject the null hypothesis if it is greater than .05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = scores['high'] - scores['low']\n",
    "# adding a column called level which indicates if impressions is \n",
    "# low or high\n",
    "ads['level'] = ads['Impressions'].apply(\n",
    "    lambda x: 'low' if x < med else 'high')\n",
    "\n",
    "\n",
    "n_repetitions = 100\n",
    "stats = []\n",
    "for i in range(n_repetitions):\n",
    "    shuffled_impres = (\n",
    "        ads['Impressions']\n",
    "        .sample(replace=False, frac=1)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    shuffled = (\n",
    "        ads\n",
    "        .assign(**{'Impressions': shuffled_impres})\n",
    "    )\n",
    "    # Dropping the level column now that we're done shuffling\n",
    "    shuffled = shuffled.drop('level', axis=1)\n",
    "\n",
    "    # Separating the data into low and high\n",
    "    med = shuffled['Impressions'].median()\n",
    "    low_ads = shuffled[shuffled['Impressions'] < med]\n",
    "    high_ads = shuffled[shuffled['Impressions'] >= med]\n",
    "\n",
    "    # Computing the accuracy for low\n",
    "    X = low_ads.drop('CountryCode', axis=1)\n",
    "    y = low_ads['CountryCode']\n",
    "\n",
    "    # Getting the accuracy score\n",
    "    accuracy = (accuracy_score(y, pl.predict(X)))\n",
    "    scores['low'] = accuracy\n",
    "\n",
    "    # Now find accuracy of high\n",
    "    X = high_ads.drop('CountryCode', axis=1)\n",
    "    y = high_ads['CountryCode']\n",
    "    # Fitting the data\n",
    "    pl.fit(X, y)\n",
    "    # Getting the accuracy score\n",
    "    accuracy = (accuracy_score(y, pl.predict(X)))\n",
    "    scores['high'] = accuracy\n",
    "    stats.append(scores['high'] - scores['low'])\n",
    "\n",
    "np.count_nonzero(stats >= obs) / n_repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
